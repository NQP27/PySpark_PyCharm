{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from Packages import sparkBuilder as sB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    spark=sB.build()  #Xây SparkSession thì mới chạy được trên PyCharm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+--------+\n",
      "| id| name| class| address|\n",
      "+---+-----+------+--------+\n",
      "|  1|Phuoc|B21CN4|ThaiBinh|\n",
      "|  2|  Loc|B21CN5|   HaNoi|\n",
      "|  3|Quang|B21CN1|VinhPhuc|\n",
      "+---+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tạo 1 df thủ công\n",
    "data=[(1,\"Phuoc\",\"B21CN4\",\"ThaiBinh\"),(2,\"Loc\",\"B21CN5\",\"HaNoi\"),(3,\"Quang\",\"B21CN1\",\"VinhPhuc\")]\n",
    "columns=['id','name','class','address']\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "df.write.csv(path=\"E:\\Worksapce\\PyCharm\\PySpark\\TempData\",header=True,mode='overwrite')\n",
    "# Spark xử lý dữ liệu phân tán nên dataframe này sẽ bị chia nhỏ thành nhiều partition\n",
    "# Nếu muốn lưu df này vào một file duy nhất thì có thể làm như sau:\n",
    "# df.repartition(1).write.csv(path=\"E:\\Worksapce\\PyCharm\\PySpark\\HandmadeData\",header=True,mode=\"overwrite\")\n",
    "# Chưa đặt tên cho file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str\n",
      "        the path in any Hadoop supported file system\n",
      "    mode : str, optional\n",
      "        specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      "            exists.\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "        for the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Write a DataFrame into a CSV file and read it back.\n",
      "    \n",
      "    >>> import tempfile\n",
      "    >>> with tempfile.TemporaryDirectory() as d:\n",
      "    ...     # Write a DataFrame into a CSV file\n",
      "    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "    ...     df.write.csv(d, mode=\"overwrite\")\n",
      "    ...\n",
      "    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "    ...     spark.read.schema(df.schema).format(\"csv\").option(\n",
      "    ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n",
      "    +---+----+\n",
      "    |age|name|\n",
      "    +---+----+\n",
      "    |100|null|\n",
      "    +---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.write.csv)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+--------+\n",
      "| id| name| class| address|\n",
      "+---+-----+------+--------+\n",
      "|  1|Phuoc|B21CN4|ThaiBinh|\n",
      "|  3|Quang|B21CN1|VinhPhuc|\n",
      "|  2|  Loc|B21CN5|   HaNoi|\n",
      "+---+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=spark.read.csv(path=\"E:\\Worksapce\\PyCharm\\PySpark\\TempData\",header=True)\n",
    "df2.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
